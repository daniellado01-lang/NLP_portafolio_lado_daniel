{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting text into sentences and words when dealing with complex real-world text containing dates, amounts, URLs, emails, acronyms, and multi-word expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a text variable, split it into:\n",
    "1. **Sentences** - logical units of meaning ending with terminal punctuation\n",
    "2. **Words (tokens)** - individual meaningful units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\n"
     ]
    }
   ],
   "source": [
    "# Sample text with challenging elements\n",
    "text = \"\"\"Dr. John Smith, Ph.D., earned $1,250.50 on Jan. 15, 2024, for his work at A.I. Corp. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "1. Dr.\n",
      "2. John Smith, Ph.D., earned $1,250.50 on Jan.\n",
      "3. 15, 2024, for his work at A.I.\n",
      "4. Corp.\n",
      "5. You can reach him at j.smith@ai-corp.co.uk or visit https://www.ai-corp.co.uk/team/dr-smith for more info.\n",
      "6. The U.S.A.-based company reported a 23.5% increase in Q3 revenue, totaling €2.5M.\n",
      "\n",
      "Words:\n",
      "['Dr', 'John', 'Smith', 'Ph', 'D', 'earned', '1', '250', '50', 'on', 'Jan', '15', '2024', 'for', 'his', 'work', 'at', 'A', 'I', 'Corp', 'You', 'can', 'reach', 'him', 'at', 'j', 'smith', 'ai', 'corp', 'co', 'uk', 'or', 'visit', 'https', 'www', 'ai', 'corp', 'co', 'uk', 'team', 'dr', 'smith', 'for', 'more', 'info', 'The', 'U', 'S', 'A', 'based', 'company', 'reported', 'a', '23', '5', 'increase', 'in', 'Q3', 'revenue', 'totaling', '2', '5M']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(r'(?<=\\.)\\s+', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def split_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return words\n",
    "\n",
    "\n",
    "sentences = split_sentences(text)\n",
    "words = split_words(text)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent}\")\n",
    "\n",
    "print(\"\\nWords:\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Tokenization Exercise\n",
    "\n",
    "This exercise explores the challenges of splitting words in large corpuses and find the most common words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Given a file `shakes.txt` in the book folder. Find the words that are more common in Shakespeare's book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common words in Shakespeare's book (first 1000 lines):\n",
      "and: 198\n",
      "the: 181\n",
      "to: 148\n",
      "of: 145\n",
      "my: 115\n",
      "in: 113\n",
      "thou: 104\n",
      "thy: 102\n",
      "s: 100\n",
      "that: 97\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def tokenize_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())  \n",
    "    return words\n",
    "\n",
    "\n",
    "with open('shakes.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_text = ''.join(line for i, line in enumerate(f) if i < 1000)\n",
    "\n",
    "all_words = tokenize_words(corpus_text)\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "most_common = word_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 most common words in Shakespeare's book (first 1000 lines):\")\n",
    "for word, count in most_common:\n",
    "    print(f\"{word}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
